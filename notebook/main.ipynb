{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e663dbb2",
   "metadata": {},
   "source": [
    "# Predicting the gender of lead actors in Hollywood films\n",
    "\n",
    "This notebook guides you step by step through the process of analyzing film data \n",
    "and building classification models to predict whether the lead actor in a movie is male or female. \n",
    "\n",
    "We'll explore the data, engineer some features, tune different machine learning models, \n",
    "and finally choose the best one to make our predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0bbf6",
   "metadata": {},
   "source": [
    "## Step 1: Importing libraries and setting up the environment\n",
    "\n",
    "This section imports the necessary Python libraries for data analysis, visualization, \n",
    "and machine learning. A random seed is also set to ensure the results are reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac8d7e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.pylabtools import figsize\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.preprocessing as skl_pre\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model as skl_lm\n",
    "import sklearn.discriminant_analysis as skl_da\n",
    "import sklearn.neighbors as skl_nb\n",
    "import sklearn.model_selection as skl_ms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e1d80",
   "metadata": {},
   "source": [
    "## Step 2: Loading the data\n",
    "\n",
    "The training and test datasets are loaded using `pandas`. This also prints out the shape \n",
    "of the training data to give an initial sense of its size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1563c922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (1039, 14)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "print(f\"Train data shape: {train.shape}\")\n",
    "\n",
    "test = pd.read_csv('../data/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4db16",
   "metadata": {},
   "source": [
    "## Step 3: Feature engineering\n",
    "\n",
    "New features are created to better capture the relationships in the data:\n",
    "\n",
    "- **Co-lead words** estimates the proportion of words spoken by the co-lead.\n",
    "- **Percentage of words spoken by female and male actors** normalizes dialogue counts.\n",
    "\n",
    "The columns `Total words` and `Difference in words lead and co-lead` are then dropped, \n",
    "since their information is incorporated into the new features. The same transformations \n",
    "are applied to both the training and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3911e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature selection and engineering ###\n",
    "\n",
    "train[\"Co-lead words\"] = (\n",
    "    (train[\"Number of words lead\"] - train[\"Difference in words lead and co-lead\"]) \n",
    "    / train[\"Total words\"]\n",
    ")\n",
    "train[\"percentage female\"] = train[\"Number words female\"] / train[\"Total words\"]\n",
    "train[\"percentage male\"] = train[\"Number words male\"] / train[\"Total words\"]\n",
    "\n",
    "train = train.drop([\"Difference in words lead and co-lead\", \"Total words\"], axis=1)\n",
    "\n",
    "# Apply the same transformations to the test data\n",
    "test[\"Co-lead words\"] = (\n",
    "    (test[\"Number of words lead\"] - test[\"Difference in words lead and co-lead\"])\n",
    "    / test[\"Total words\"]\n",
    ")\n",
    "test[\"percentage female\"] = test[\"Number words female\"] / test[\"Total words\"]\n",
    "test[\"percentage male\"] = test[\"Number words male\"] / test[\"Total words\"]\n",
    "\n",
    "test = test.drop([\"Difference in words lead and co-lead\", \"Total words\"], axis=1)\n",
    "\n",
    "X = train.drop(\"Lead\", axis=1)\n",
    "y = train[\"Lead\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e0733",
   "metadata": {},
   "source": [
    "# Finding the optimal *k* for k-Nearest Neighbors\n",
    "\n",
    "To tune our k-NN model, we run multiple cross-validation loops over a range of *k* values (from 1 to 200).\n",
    "This helps us understand how the choice of *k* affects prediction accuracy.\n",
    "\n",
    "We repeat this process several times to get a stable estimate, then plot the average accuracy\n",
    "to identify the best *k* value for our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "### KNN METHOD ###\n",
    "# Cross-validation with repeated splitting to find the optimal k\n",
    "\n",
    "n_runs = 10\n",
    "K = np.arange(1, 200)\n",
    "classification = np.zeros((n_runs, len(K)))\n",
    "\n",
    "for i in range(n_runs):\n",
    "    x_train, x_val, y_train, y_val = skl_ms.train_test_split(\n",
    "        X, y, test_size=0.3, random_state=1\n",
    "    )\n",
    "    \n",
    "    # Rescale data with Standard Scaler\n",
    "    scaler = skl_pre.StandardScaler().fit(x_train)\n",
    "    \n",
    "    for j, k in enumerate(K):\n",
    "        model = skl_nb.KNeighborsClassifier(n_neighbors=k)\n",
    "        model.fit(scaler.transform(x_train), y_train)\n",
    "        prediction = model.predict(scaler.transform(x_val))\n",
    "        classification[i, j] = np.mean(prediction == y_val)\n",
    "\n",
    "average_classification = np.mean(classification, axis=0)\n",
    "\n",
    "optimal_k_index = np.argmax(average_classification)\n",
    "optimal_k = K[optimal_k_index]\n",
    "optimal_accuracy = average_classification[optimal_k_index]\n",
    "\n",
    "print(f\"The optimal k is {optimal_k} with an average accuracy of {optimal_accuracy:.4f}\")\n",
    "\n",
    "plt.scatter(K, average_classification)\n",
    "plt.xlabel(\"k (number of neighbors)\")\n",
    "plt.ylabel(\"Average classification accuracy\")\n",
    "plt.title(\"Finding the optimal k for k-NN\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d2c89",
   "metadata": {},
   "source": [
    "# Tuning Random Forest hyperparameters\n",
    "\n",
    "Next, we tune the hyperparameters of the Random Forest classifier to improve its performance.\n",
    "We use `GridSearchCV` to explore different values for:\n",
    "\n",
    "- `n_estimators`: the number of trees in the forest.\n",
    "- `max_leaf_nodes`: the maximum number of leaf nodes in each tree.\n",
    "\n",
    "We use 5-fold cross-validation to evaluate the performance of each parameter combination and\n",
    "print out the best configuration along with its cross-validated accuracy score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b61c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': list(range(10, 100, 10)),\n",
    "    'max_leaf_nodes': list(range(10, 100, 10)),\n",
    "    'random_state': [1]\n",
    "}\n",
    "\n",
    "grid_search = skl_ms.GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(f\"Best cross-validated accuracy: {grid_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed5102",
   "metadata": {},
   "source": [
    "# Comparing different models with k-fold cross-validation\n",
    "\n",
    "To understand how our selected models perform relative to each other, we run a k-fold cross-validation \n",
    "process (with 5 folds) and calculate the validation error for each model. This gives us a clearer picture \n",
    "of which models are more stable and accurate on unseen data.\n",
    "\n",
    "We include:\n",
    "\n",
    "- Quadratic Discriminant Analysis (QDA)\n",
    "- k-Nearest Neighbors with the optimal *k* we found\n",
    "- Random Forest with tuned hyperparameters\n",
    "\n",
    "Finally, we plot a boxplot of the validation errors for each model to visualize their performance spread.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to compare\n",
    "models = []\n",
    "models.append(skl_da.QuadraticDiscriminantAnalysis())\n",
    "models.append(skl_nb.KNeighborsClassifier(n_neighbors=optimal_k))\n",
    "models.append(RandomForestClassifier(\n",
    "    n_estimators=grid_search.best_params_['n_estimators'],\n",
    "    max_leaf_nodes=grid_search.best_params_['max_leaf_nodes'],\n",
    "    random_state=1\n",
    "))\n",
    "\n",
    "# k-fold cross-validation\n",
    "n_fold = 5\n",
    "misclassification = np.zeros((n_fold, len(models)))\n",
    "cv = skl_ms.KFold(n_splits=n_fold, random_state=2, shuffle=True)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv.split(X)):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    for j, model in enumerate(models):\n",
    "        scaler = skl_pre.StandardScaler().fit(X_train)\n",
    "        model.fit(scaler.transform(X_train), y_train)\n",
    "        prediction = model.predict(scaler.transform(X_val))\n",
    "        misclassification[i, j] = np.mean(prediction != y_val)\n",
    "\n",
    "# Boxplot to compare validation errors\n",
    "plt.boxplot(misclassification)\n",
    "plt.xticks(np.arange(len(models)) + 1, [type(model).__name__ for model in models])\n",
    "plt.ylabel(\"Validation Error\")\n",
    "plt.title(\"Model comparison with k-fold cross-validation\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ffb4df",
   "metadata": {},
   "source": [
    "# Training the final model and generating predictions\n",
    "\n",
    "Based on our cross-validation results, we choose the Quadratic Discriminant Analysis (QDA) model \n",
    "as our final model for production. We train it on the entire training dataset to leverage all available data.\n",
    "\n",
    "Finally, we predict on the test set and save the results in a CSV file named `predictions.csv`,\n",
    "formatted as required for submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final QDA model trained on full training data\n",
    "final_model = skl_da.QuadraticDiscriminantAnalysis()\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Make predictions on test set\n",
    "final_predictions = final_model.predict(test)\n",
    "\n",
    "# Convert to 0/1: 0 for Male, 1 for Female\n",
    "final_predictions = np.where(final_predictions == \"Male\", 0, 1).reshape(1, -1)\n",
    "\n",
    "# Save to predictions.csv without index and header\n",
    "pd.DataFrame(final_predictions).to_csv(\"../results/predictions.csv\", index=False, header=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
